% !TEX root=foo.tex
\section{Method}
\label{sec:method}
% This study involved collection of data from a popular technology news site, Slashdot. We collected 1000 comments related to Facebook and use. we then implemented a multi-level human coding to probe into why Facebook usage is reducing everyday. We chose Slashdot to particularly focus on the techies. Often the techies are removed from data collection to obtain an unbiased sample. However the people concerned about their privacy and security online are increasing everyday and they are playing a vital role in altering the user base of social media websites like- Facebook. We also applied topic modelling and sentimental analysis to avoid the limtiation of only human coding.

In this section, we discuss how we performed two level of codings followed by the topic modeling, sentiment analysis, and wordcloud generation methods:

\subsection{Data Collection}
We compiled a new corpus of articles and related comments from a technology news site: Slashdot. This site was chosen for several reasons: Slashdot was voted one of Newsweek's favorite technology web sites and rated one of Yahoo's top 100 web sites as "Best Geek Hangout".\footnote{\url{http://digitalenterprise.org/cases/slashdot.html}} So we can infer that posters and commenters of Slashdot are tech-savvy users and have better insight of the pros and cons of state of the art technology. Therefore, their reviews are critical in understanding the existing \emph{Non-use} of social media (Facebook in our case) and related sentiments. Also, we were looking for a discussion based blog or forum from where we can get unprompted naturalistic responses. Comments on Slashdot are moderated by users of the site, each comment has a score between -1 to +5 which ensures high quality contribution and discouraging spams or vandalism. Each moderator in Slashdot is able to modify the score of a given comment by +1 or -1. Furthermore, each comment is classified in several categories: for good comments, the classes are: Interesting, Insightful, Informative, and Funny. For bad comments, the classes are: Flamebait, Troll, Off-Topic, and Redundant. Posts are associated with topic tags, such as political, facebook, social media, science etc. 

Slashdot comments are displayed in a threaded tree type discussion conversation. Commenters can reply in a given thread and subsequent replies for that comment appear in hierarchical order. We scraped the whole history of posts and associated comments since the inception of Facebook (2004) till date. We used Python \textit{BeautifulSoup} and \textit{Scrapy} for crawling and the data was saved into MongoDB in JSON format. For our purpose of analysis, we only considered the posts which have a topic tag "Facebook". The comments that we collected for analysis have been filtered based on the following factors: 
\begin{itemize}
    \item contains the term "Facebook", "FB" or "Social Media" in body or title for relevance.
    \item minimum 100 characters in length for better understanding of context.
    \item minimum score of +2 to ensure high quality comments.
\end{itemize}
\subsection{Binary Coding}
A random sample of 1000 comments that satisfy the aforementioned criteria were collected from the whole dataset. Later, they were manually coded by two independent coders (first and second author) to check if they belong to either one of the class \textit{Non-use} or \textit{not related to Non-use}. If a particular comment reflected harsh criticism of Facebook, inactive or passive usage, reduced usage or complete abandonment: the coders classified that comment as \emph{Non-use}. Other comments which discussed all other different topics related to Facebook were marked as \textit{not related to Non-use}. One typical such example of \emph{Non-use} comment is:
\begin{quote}
    \textit{The future does not look so bright for facebook as they will probably suffer the same fate as usenet. Sounds like a positive outcome to me. The world would be better off without the cancer that is facebook.}

\end{quote}

To make sure both the coders are on same wavelength, we checked the agreement after coding 500 comments. It turned out we achieved 78.3\% agreement with the Cohen's $\kappa$=0.56. Later, we checked a subset of 20 of the disagreements and had a discussion to resolve the discrepancies and come to an agreement. Then we individually re-examined all other disagreement cases and checked if we wanted to keep or change our original classification. It was discovered that the raters encountered one major point where both of them disagreed mostly: initially all the comments that explicitly stated or expressed some form of \emph{Non-use}, such as "deleted my account...", "not using for long...", "I hate Facebook..." etc. were marked as \textit{Non-use} only by one rater while the other rater also insisted to incorporate comments related to harsh criticism or sarcasm of Facebook. This was reasonable since in such cases users might not explicitly mention about the \textit{Non-use} but their aversion towards Facebook is visible which might lead them or encourage other users to \textit{Non-use}.  After the discussion on the remaining disagreements, the agreement rate was 92.8\% with the Cohen's $\kappa$=0.856. After coding all 1000 comments, the overall agreement rate was 93.6\% with the Cohen's $\kappa$=0.872. Table~\ref{tab:table1} summarizes the categories assigned by the raters. Only 64 (6.4\%) times the raters disagreed. Since we got a high agreement score, we picked 516 (452+21+43) comments which were marked as \textit{Non-use} by one or both the raters for second level thematic coding.


\newcommand{\head}[1]{\textnormal{\textbf{#1}}}

\begin{table}[!ht]
% let LaTeX figure out optimal amount of intercolumn whitespace:
\setlength\tabcolsep{0pt} 

\begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}c*{3}{T{1.8}}cT{1.8}}  
\toprule
\head{First/Second Rater} & \head{NU} & \head{NRNU}\\
\midrule
\textbf{NU}              & 452     & 21 \\                    
   \textbf{NRNU} & 43      & 484 \\
  \hline
\bottomrule 
\end{tabular*}
\caption{Categories assigned by two independent raters on 1000 comments. (NU = Non-use, NRNU = Not Related to Non-Use)}
    \label{tab:table1}
\end{table}
\subsection{Thematic Coding}
After coding 516 comments in \emph{Non-use} category as discussed above, we wanted to expand more on finding the reasons of \emph{Non-use} by the Facebook users. We performed a second level thematic coding by implementing the coding categories of Das and Patil's paper~\cite{das2017resistance}. We could not extrapolate all the categories generated from that paper due to nature of differences in our data. We observed a subset of 7 coding themes which encompassed all of our comments and the remaining did not appear. Another exclusive category that appeared but we did not report was \textit{Modified Behavior}, because comments belonging to this category reflected change in user interaction with Facebook but it was more of an effect than a cause. One such comment is:

\begin{quote}
    \textit{I left facebook a few months ago and specifically requested deletion, not deactivation. There was a 14 day waiting period, during which time I could log back into my account and reset the clock, but supposedly at the end of those 14 days my account was gone for good. From what I can tell [facebook.com] they still allow you to do this: "If you don't think you'll use facebook again, you can request to have your account permanently deleted. Please keep in mind that you won't be able to reactivate your account or retrieve anything you've added."}

\end{quote}

However, we keep those comments for study as they were marked as \textit{Non-use} in the first place. But for the purpose of simplicity of explanation and NLP analysis, we will count it as the eighth theme. Table~\ref{table:coding category} describes these 7 themes implemented by the authors of this paper, number of comments belonging to that theme and one example comment. The theme titles are self-explanatory yet we give some keywords or topic words (second column) per theme which will help elucidate the coding criteria.    

% . From our comments, we coould classify some of the comments signifying having a Facebook account or not, however we cannot classify accordingly for all the categories. Thus, we chose a subset of the coding categories which consisted on 7 coding categories were generated by exploring the qualtitaitve study of interviews. Table~\ref{table:coding category} describes the coding categories implemented by the authors of this paper and a few examples as discussed later.

The two coders who coded the binary coding, coded a first set of 100 comments of the 516 \emph{Non-use} comments.  Initially we only allowed a single code per comment for the first set of coding. We noted the inter-coder reliability score was 70.4\% with the Cohen's $\kappa$=0.637. The rating was fairly good, however we discussed about the coding scheme and noted that due to the nature of some of the comments we could not classify one comment only under a single theme. After discussing further, we decided on coding a single comment for multiple themes. For example, the following quote by one of the commenters not only shows that they are not using Facebook due to Facebook's feature of tagging users (indicating Facebook functionality influence as user dissatisfaction) but also indicates about the audience activity of tagging users. Both the users coded this comment under \textit{Audience} as well as \textit{Facebook Functionality}.
\begin{quote}
\textit{I don't use Facebook. That doesn't stop my friends from tagging my face when I'm in one of their photos which they post on Facebook.}
\end{quote}
After the thematic coding of a total of 100 comments, we found the inter-coder reliability score of 89.8\% with the Cohen's $\kappa$=0.87. With this inter-coder reliability score, we coded the rest of the comments and found that the inter-coder reliability score after coding all the 516 was 97.3\% with the Cohen's $\kappa$=0.966. The thorough coding not only enabled the coders to analyze the non-usage aspect of Facebook users but also indicated the specific reasons on why the users are behaving in a certain way leading to Facebook rejection of some form. 

\subsection{Wordcloud and Topic Modeling}
We performed NLP based analysis to get a sense of the innate sentiment of the commenters as well as the reasons for their deliberate \textit{Non-use}. Wordclouds have been particularly useful to get a top level idea and has been widely used for text mining \cite{younis2015sentiment}. We picked the 516 comments those were marked as \textit{Non-use} and built wordcloud using Python \textit{wordcloud} package. The initial wordcloud had irrelevant terms which are not informative, such as "facebook", "would', "say" etc. Therefore, we created a custom list of stopwords ourselves besides the Python package \textit{nltk} provided list of stopwords. To get better insight, we created bigram and trigram wordcloud on the same dataset using \textit{tm} library of R. Data was preprocessed and cleaned using process described in ~\cite{wang2013gender}. While we removed the stopwords for unigram and bigram wordclouds, it was not done for trigrams as we wanted phrases like "i hate facebook", "i don't care" etc. The results are dicussed in the following section.

Besides wordclouds we performed Latent Dirichlet Allocation (LDA) topic modeling \cite{blei2003latent} to surface the hidden topics (sentiment words, non-use related terms) from the comments. LDA is a statistical method that assumes a corpus is a mixture of topics and a topic is mixture of words ~\cite{suresh2015autodetection,wang2013gender}. We used the python library \textit{Gensim} and trained the model on 516 \textit{Non-use} comments after cleaning the data (removing punctuation, proper names, tokenizing and lemmatizing). The number of topics were chosen as $K=8$ for two reasons: we got 8 factors from the thematic coding and the topics started repeating after this threshold hence becoming less interpretable. We made sure to eliminate words that only appeared a few times and generic words that appeared several times. This helped improving the information gain and finding better topics by fast convergence. Each topic is associated with weighted words under that topic and the high weight represent most depictive word of that particular topic. 

To confirm our results of LDA topics, we implemented another popular topic modeling approach called Non-negative Matrix Factorization (NMF) because it is better at compact semantic representation of documents in small data setting ~\cite{choo2013utopian}. Using tf-idf ("term frequency-inverse document frequency") from \textit{scikit-learn}, we converted each document in corpus to vectors. One important thing to note is, we had to set minimum and maximum document arguments to get rid of too generic or too unlikely words. The number of topics were chosen as 8 for the similar reasons mentioned above.

\subsection{Sentiment Analysis}

To understand the polarity of sentiments of users, we performed sentiment analysis on the comments. We used VADER (Valence Aware Dictionary and Sentiment Reasoner), a lexicon and rule-based sentiment analysis tool that is specifically engineered to extract sentiments expressed in social media. It incorporates qualitative and quantitative methods and performs as well as other standard benchmarks for sentiment analysis (e.g., LIWC, ANEW, the General Inquirer, and machine
learning based techniques) ~\cite{hutto2014vader}. We performed overall and theme wise (8 influential factors mentioned above) sentiment analysis using \textit{vaderSentiment} package in Python and reported the compound sentiment score and associated sentiment (positive or negative). One important adjustment that we made while doing this analysis was: we filtered out non sentiment related terms and picked the opinion terms instead. This technique was proposed by Pang and Lee ~\cite{pang2004sentimental} to remove objective sentences by extracting subjective ones. Often times in lexicon based sentiment analysis in a review or comment (especially long ones), it is observed that the non-sentiment related sentences affect the overall sentiment polarity of the review. To overcome this, we adopted a simple approach to filter the opinion sentences from the corpus. We took advantage of the list of positive and negative words compiled by Hu and Liu~\cite{hu2004mining} (2005 positive and 4783 negative words). From each comment, the sentences that contain at least one sentiment word from above list (positive or negative) were retained. We call them the \emph{opinion sentences}. The remaining analysis was routinely performed using VADER tool, and  the categorization of scoring and interpretation of results are given in the following section.

% In order to get contextual polarity from the comments and compare our results with findings obtained above, we also trained a logistic regression model. This is a simple linear model and is widely used for sentiment classification. To train the model, we used the data provided by Kaggle\footnote{\url{https://inclass.kaggle.com/c/si650winter11}}. Every training data is a sentence extracted from social media (blogs). The training data contains 7086 sentences, already labeled with 1 (positive sentiment) or 0 (negative sentiment). With help of Python \textit{scikit-learn}, we converted the corpus into matrix of tokens (after all preprocessing). Then we performed logistic regression using 85\% of input data for training and remaining 15\% for validation (for accuracy estimation). The trained model was applied on our theme wise comment set for sentiment classification and the findings are discussed in Findings section.
 



