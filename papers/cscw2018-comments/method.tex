% !TEX root=nonusecomments.tex
\section{Method}
\label{sec:method}
% This study involved collection of data from a popular technology news site, Slashdot. We collected 1000 comments related to Facebook and use. we then implemented a multi-level human coding to probe into why Facebook usage is reducing everyday. We chose Slashdot to particularly focus on the techies. Often the techies are removed from data collection to obtain an unbiased sample. However the people concerned about their privacy and security online are increasing everyday and they are playing a vital role in altering the user base of social media websites like- Facebook. We also applied topic modelling and sentimental analysis to avoid the limtiation of only human coding.
We collected comments on Facebook related blog posts and associated reader comments from two technology oriented blogs, viz., Slashdot and Schneier on Security. We performed two-stage coding on a random sample of these comments to uncover negative sentiment as well as implied or explicit indication of non-use. In the following subsections, we describe our data collection, comment selection, and coding.
%We cleaned the collected data and performed empirical qualitative coding on the collected set of comments. The coding was performed in three phases. First, to find out which comments where directed in the direction of negative sentiments towards social media specifically Facebook. Next, focusing on whether such negative sentiments evolved into non-usage or users still used Facebook irrespective of their negative sentiments. In the third level of coding we focused primarily on finding the reasons behind the negative connotations towards Facebook.
\subsection{Data Collection}
For both of our selected data sources, we scraped the respective sites for all posts related to Facebook since Facebook first started in 2004. We also collected all reader comments for the scraped posts. We scraped the sites using Python packages \textit{BeautifulSoup} and \textit{Scrapy} and saved the data in MongoDB in JSON format.

\subsubsection{Slashdot}
Posts on Slashdot are associated with topic tags, such as Political, Facebook, Social media, Science, etc. We collected 940 Slashdot posts with the topic tag "Facebook" with 78,597 reader comments across these posts.

Reader comments on a Slashdot post are displayed as a threaded discussion. Commenters can reply to a given thread and subsequent replies for that comment appear in hierarchical order. To ensure content quality and flag spam or vandalism, comments are moderated by the readers of the site. Each moderator in Slashdot is able to modify the score of a given comment by +1 or -1, and each comment has a cumulative score between -1 to +5. Further, each comment is classified into one of several categories: Interesting, Insightful, Informative, and Funny for good comments and Flamebait, Troll, Off-Topic, and Redundant for bad comments.


%Add the extra stats - SD
%We compiled a corpus of articles and related comments from a technology news site: Slashdot. This site was chosen for several reasons: Slashdot was voted one of Newsweek's favorite technology web sites and rated one of Yahoo's top 100 web sites as "Best Geek Hangout".\footnote{\url{http://digitalenterprise.org/cases/slashdot.html}}. In general, we can infer that posters and commenters of Slashdot are tech-savvy users and have better insight of the pros and cons of state of the art technology. Therefore, their reviews are critical in understanding the existing \emph{Non-use} of social media (Facebook in our case) and related negative sentiments. Also, we were looking for a discussion based blog or forum from where we can get unprompted naturalistic responses. Comments on Slashdot are moderated by users of the site, each comment has a score between -1 to +5 which ensures high quality contribution and discouraging spams or vandalism. Each moderator in Slashdot is able to modify the score of a given comment by +1 or -1. Furthermore, each comment is classified in several categories: for good comments, the classes are: Interesting, Insightful, Informative, and Funny. For bad comments, the classes are: Flamebait, Troll, Off-Topic, and Redundant. Posts are associated with topic tags, such as political, facebook, social media, science etc. This strict, informative moderation and communicative environment makes Slashdot an ideal source of study \cite{halavais2001slashdot}. 

%Slashdot comments are displayed in a threaded tree type discussion conversation. Commenters can reply in a given thread and subsequent replies for that comment appear in hierarchical order. We scraped the whole history of posts and associated comments since the inception of Facebook (2004) till date. We used Python \textit{BeautifulSoup} and \textit{Scrapy} for crawling and the data was saved into MongoDB in JSON format. For our purpose of analysis, we only considered the posts which have a topic tag "Facebook" (a total of 78,597 comments comprised this database). The comments that we collected for analysis have been filtered based on the following factors: 
%\begin{itemize}
%    \item contains the term "Facebook", "FB" or "Social Media" in body or title for relevance.
%    \item minimum 100 characters in length for better understanding of context.
%    \item minimum score of +2 to ensure high quality comments.
%\end{itemize}

\subsubsection{Schneier on Security}
Posts on Schneier on Security are written by well-known security professional Bruce Schneier with blog readers able to leave comments without needing to sign up. The comments are moderated, thus ensuring that off-topic or spam comments are rare.

We initially selected posts with the tags ``Facebook'' and ``Social Media'' ending up with only 69 matching articles since 2004. However, selecting based on the presence of these two terms in the article title and content yielded a large number of article irrelevant for the topic of our research. In the end, we examined all articles since 2004 and manually selected 237 posts related to social media. Across the 237 posts, 11365 comments were posted by blog readers.

%One of the other blogs we targeted for data collection was "Security Guru" Bruce Schneier's blog (Schneier on Security) where the articles mainly focus on topics such as "privacy", "security", surveillance", "hacking", "national security policy", "data breaches", "terrorism" and "social media". Articles are posted by Schneier himself with users given the feature to express their views (without a need to sign up). The comments are moderated on a regular basis, hence the probability of finding off-topic and spam comments are pretty rare. For the purposes of our research, we initially limited the filtering to the tags of the articles only from the year 2004 onwards, using the keywords ["Facebook", "social media"] that yielded a low number of articles (69) which seemed inadequate for analysis. We then extended the filter on the "title" and "article content" fields as well to obtain a significant number of articles. The filter on article content introduced a lot of redundant articles, due to which we read every article and manually annotated them as being social media related or not. We obtained 237 such articles and 11365 comments corresponding to it with this method. Now, we filter the comments based on keywords ["Facebook", "Social Media", "FB"] such that length of each comment is at least 100 characters. We obtain a total of 1156 comments by this method. An interesting observation was that we additionally obtained a list of 2677 comments other than the list of 1156 that do fit the criteria of having one of the keywords ["Facebook", "Social Media", "FB"] and length greater than 100 characters, however this was from the overall database (not only from the ones manually annotated by the authors as social media related). We used the the database of 1156 comments for relevance to social media.

\subsection{Comment Selection}
From the collected large dataset of reader comments on relevant posts from Slashdot and Schneier on Security, we selected 3000 comments for deeper examination and analysis, 2000 from Slashdot and 1000 from Schneier on Security, respectively.

First, we selected comments from our dataset that were at least 100 characters in length and contained one or more of the terms ``Facebook,'' ``FB,'' or ``Social media.'' For Slashdot comments, we further narrowed the set to include only those comments with a score of +2 or higher. These criteria ensured that the set of selected comments were of sufficiently high quality and included enough content to allow understanding. We analyzed 2000 comments from Slashdot and 1000 comments from Schneier on Security chosen randomly from this set.

\subsection{Coding}
We coded the set of 3000 comments in two stages:

\subsubsection{Sentiment coding}
The first and the second author independently marked whether each of the comments expressed negative sentiment \emph{(NS)} about Facebook, such as criticism, reduced usage, abandonment, etc. For example, the following comment was classified as expressing negative sentiment about Facebook: \textit{``The future does not look so bright for Facebook as they will probably suffer the same fate as Usenet. Sounds like a positive outcome to me. The world would be better off without the cancer that is Facebook.''}

We chose a broad examination negative sentiment instead of a narrow focus on non-use for a number of reasons. First, intention and sentiment are strong indicators of behavior; negative sentiment is thus likely to drive and indicate non-use. Second, comments that expressed negative sentiment may strongly imply non-use without mentioning explicit practices. Third, negative sentiment could describe or prescribe non-use by people other than the commenter. Fourth, comments with negative sentiment expand the dataset and enable a richer and diverse capture of the pulse of the reader population regarding the practices related to the technology in question.

We initially coded 500 comments from Slashdot and achieved 78.3\% agreement between the two coders with Cohen's $\kappa$=0.56. Next, we discussed and resolved 20 of the discrepancies and revisited the initial coding in light of the discussion reaching an agreement rate of 92.8\% with Cohen's $\kappa$=0.856. After confirming high agreement for the initial 500 comments, we independently coded the remaining 2500 comments, achieving an overall agreement rate was 93.6\% with the Cohen's $\kappa$=0.872.

In the end, we found that negative sentiment toward Facebook was expressed in 1342 of the comments, 1022 from Slashdot and 320 from Schneier on Security, respectively.

\subsubsection{Thematic coding}
Next, we utilized techniques from Grounded Theory~\cite{draucker2007theoretical} to engage in an inductive and comprehensive qualitative thematic analysis~\cite{joffe2004content} of the 1342 comments with negative sentiment toward Facebook. The first and the second authors generated codes via open coding followed by axial and selective coding to identify relationships and thematic groupings among the codes. Table~\ref{table:coding category} shows the themes that emerged from our coding.

After finalizing the themes, the two coders independently categorized each of the 1342 comments with negative sentiment toward Facebook into the themes. The themes are not mutually exlusive; i.e., each comment was coded as belonging to all applicable themes. For example, consider the comment \textit{``Maybe I'll see no ads after changing my Facebook age to something ridiculous? Why does anybody put their real age/birthday into Facebook?''} This comment was coded under the themes \textit{Advertisement} and \textit{Privacy and security concerns}.

We found that inter-coder reliability was 89.8\% with the Cohen's $\kappa$=0.87 for an initial set of 100 comments. Given the high inter-coder reliability, we proceeded to code the rest of the set, reaching an overall inter-coder reliability score 97.3\% with the Cohen's $\kappa$=0.966. Table~\ref{table:coding category} provides the number of comments for each of the themes.
%The key idea behind picking all the comments those were related to \emph{NS} instead of just the ones expressing \textit{Non-use} was to capture the overall negative emotions of the users with a special focus on \textit{Non-use}. Also that allowed us to get a bigger dataset for thematic coding.  We also classified individual comments into \textit{Implicit Negative Sentiment}, \emph{INS}. Simultaneously, individual comments were marked as \textit{explicit Non-use} if they directly express passive use, reduced use or non-use (e.g., "...I don't have an account..", "...I have stopped using Facebook.." etc.). Both the categories formed our set of negative sentiment towards Facebook. Next, we wanted to find how the underlying factors or themes appear in these two disjoint sets of \emph{NS}.


%\subsection{Binary Coding}
%A random sample of 2000 comments from Slashdot and 1000 comments from Schneier's blog that satisfy the aforementioned criteria were collected from the whole dataset. Later, the collected comments were manually coded by two independent coders (first and second author) to check if they belong to either one of the class Facebook Negative Sentiment \emph{(NS)} or \textit{not related to NS}. If a particular comment reflected harsh criticism of Facebook, inactive or passive usage, reduced usage or complete abandonment: the coders classified that comment as \emph{NS}. Other comments which discussed all other different topics related to Facebook were marked as \textit{not related to NS}. One typical such example of \emph{NS} comment is:
%\begin{quote}
%    \textit{The future does not look so bright for facebook as they will probably suffer the same fate as usenet. Sounds like a positive outcome to me. The world would be better off without the cancer that is facebook.}
%\end{quote}

%To make sure both the coders are on same wavelength, we checked the agreement after coding 500 comments from Slashdot. It turned out we achieved 78.3\% agreement with the Cohen's $\kappa$=0.56. Later, we checked a subset of 20 of the disagreements and had a discussion to resolve the discrepancies and come to an agreement. Then we individually re-examined all other disagreement cases and checked if we wanted to keep or change our original classification. It was discovered that the raters encountered one major point where both of them disagreed mostly: initially all the comments that explicitly stated or expressed some form of \emph{NS}, such as "deleted my account...", "not using for long...", "I hate Facebook..." etc. were marked as \textit{NS} only by one rater while the other rater also insisted to incorporate comments related to criticism, concern or sarcasm of Facebook. One such example is:
%\begin{quote}
%\textit{Make with the transparency. How do the stream sorting algorithms work? If Facebook can't divulge that, I see no reason to trust them.}
%\end{quote}

%This was reasonable since in such cases users might not explicitly mention about the \textit{NS} but their aversion towards Facebook is visible which might lead them or encourage other users to \textit{Non-use}. After the discussion on the remaining disagreements, the agreement rate was 92.8\% with the Cohen's $\kappa$=0.856. After coding all 3000 comments, the overall agreement rate was 93.6\% with the Cohen's $\kappa$=0.872. 

%\subsection{Thematic Coding}
%After coding total 3000 comments from both data sources as discussed above, we wanted to expand more on finding the reasons of \emph{NS} and \emph{Non-use} by the Facebook users. It is worth noting that initially we just filtered the Facebook \emph{NS} related comments. Following the grounded theory (GT) approach \cite{draucker2007theoretical}, an inductive and comprehensive qualitative thematic analysis \cite{joffe2004content} was done on the overall comments from both data source which were marked as \emph{NS} related in the initial binary coding. Initially we wanted to do separate analysis but it turned out the themes overlap so it was not required. All the \emph{NS} comments were read thoroughly for potential codes or themes by open coding (by the two authors). In general, similar comments were clustered together which were commonly mentioned by the users. Following GT's constant comparison and axial coding scheme, the underlying factors were first identified, compared and later grouped together to constitute individual themes. 

%The key idea behind picking all the comments those were related to \emph{NS} instead of just the ones expressing \textit{Non-use} was to capture the overall negative emotions of the users with a special focus on \textit{Non-use}. Also that allowed us to get a bigger dataset for thematic coding.  We also classified individual comments into \textit{Implicit Negative Sentiment}, \emph{INS}. Simultaneously, individual comments were marked as \textit{explicit Non-use} if they directly express passive use, reduced use or non-use (e.g., "...I don't have an account..", "...I have stopped using Facebook.." etc.). Both the categories formed our set of negative sentiment towards Facebook. Next, we wanted to find how the underlying factors or themes appear in these two disjoint sets of \emph{NS}.
%We will explain here how we used another level of classification on this \emph{NS} comments to distinguish the  \textit{explicit Non-use} only comments and remaining \emph{implicit NS} comments. This approach is also justified since it allowed us to observe how the underlying factors or themes appear in these two disjoint sets.
% . From our comments, we coould classify some of the comments signifying having a Facebook account or not, however we cannot classify accordingly for all the categories. Thus, we chose a subset of the coding categories which consisted on 7 coding categories were generated by exploring the qualtitaitve study of interviews. Table~\ref{table:coding category} describes the coding categories implemented by the authors of this paper and a few examples as discussed later.

%The two coders who coded the binary coding, coded a first set of 100 comments from all the \emph{NS} comments. Initially we only allowed a single code per comment for the first set of coding. We noted the inter-coder reliability score was 70.4\% with the Cohen's $\kappa$=0.637. The rating was fairly good, however we discussed about the coding scheme and noted that due to the nature of some of the comments we could not classify one comment only under a single theme. After discussing further, we decided on coding a single comment for multiple themes. For example, the following quote by one of the commenters not only shows that they are disliking ads on Facebook but also indicates their lack of trust to this media. Both the coders coded this comment under \textit{Advertisement} as well as \textit{Privacy and security concerns}.
%\begin{quote}
%\textit{Maybe I'll see no ads after changing my Facebook age to something ridiculous? Why does anybody put their real age/birthday into Facebook?}
%\end{quote}

%Table~\ref{table:coding category} shows the different coding themes the researchers developed through rigorous open, axial, and later thematic coding. After the thematic coding of a total of 100 comments, we found the inter-coder reliability score of 89.8\% with the Cohen's $\kappa$=0.87. With this inter-coder reliability score, we coded the rest of the comments and found that the inter-coder reliability score after coding all the \emph{NS} comments was 97.3\% with the Cohen's $\kappa$=0.966. The iterative and independent assignment of the codes followed by the review ensured trustworthiness. Table~\ref{table:coding category} summarizes the 10 themes those were surfaced with their relative frequency.
% In order to get contextual polarity from the comments and compare our results with findings obtained above, we also trained a logistic regression model. This is a simple linear model and is widely used for sentiment classification. To train the model, we used the data provided by Kaggle\footnote{\url{https://inclass.kaggle.com/c/si650winter11}}. Every training data is a sentence extracted from social media (blogs). The training data contains 7086 sentences, already labeled with 1 (positive sentiment) or 0 (negative sentiment). With help of Python \textit{scikit-learn}, we converted the corpus into matrix of tokens (after all preprocessing). Then we performed logistic regression using 85\% of input data for training and remaining 15\% for validation (for accuracy estimation). The trained model was applied on our theme wise comment set for sentiment classification and the findings are discussed in Findings section.